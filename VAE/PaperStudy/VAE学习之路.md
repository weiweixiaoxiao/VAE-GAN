# VAE学习之路

主要的论文Auto-Encoding Variational Bayes.

论文下载地址：http://arxiv.org/abs/1312.6114

“该论文提出一个融合变分贝叶斯（Variational Bayes）方法和神经网络的方法，这个方法可以用来构造生成模型的自编码器。”

这句话中提到--变分贝叶斯，先解释一下什么事变分贝叶斯。

## 变分贝叶斯

### 贝叶斯

 先验分布、后验分布、似然函数、贝叶斯公式

**先验分布**： 先验概率仅仅依赖于主观上的经验估计，也就是事先根据已有的知识的推断，先验概率就是没有经过实验验证的概率，根据已知进行的主观臆测。

​                                                     `如抛一枚硬币，在抛之前，主观推断P（正面朝上） = 0.5。`

**后验分布：** 后验概率是指在得到“结果”的信息后重新修正的概率，如贝叶斯公式中的。是“执果索因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。解释下来就是，在已知果（B）的前提下，得到重新修正的因（A）的概率P（A|B），称为A的后验概率，也即条件概率。后验概率可以通过贝叶斯公式求解。

**似然函数：** 在[数理统计学](http://zh.wikipedia.org/wiki/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%AD%A6)中，**似然函数**是一种关于[统计模型](http://zh.wikipedia.org/w/index.php?title=%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B&action=edit&redlink=1)中的[参数](http://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0)的[函数](http://zh.wikipedia.org/wiki/%E5%87%BD%E6%95%B0)，表示模型参数中的**似然性**。是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

```
对于“一枚正反对称的硬币上抛十次”这种事件，我们可以问硬币落地时十次都是正面向上的“概率”是多少；
而对于“一枚硬币上抛十次，落地都是正面向上”这种事件，我们则可以问，这枚硬币正反面对称（也就是正反面概率均为0.5的概率）的“似然”程度是多少。
```



### 变分贝叶斯

**解释知识点最全的网址**：http://blog.huajh7.com/2013/03/06/variational-bayes/

​        通常在研究贝叶斯模型中，很多情况下我们关注的是如何求解后验概率(Posterior)，不幸的是，在实际模型中我们很难通过简单的贝叶斯理论求得后验概率的公式解，但是这并不影响我们对贝叶斯模型的爱——既然无法求得精确解，来个近似解在实际中也是可以接受的:-)。

一般根据近似解的求解方式可以分为：1 随机(Stochastic)近似方法（代表是MCMC，利用Gibbs Sampling训练LDA的模型便是一种）。2 另外一种确定性(Deterministic)近似方法。本篇要介绍的变分推断便属于后者，一般情况下确定性近似方法会比随机近似方法更快和更容易判断收敛。变分贝叶斯推断是一种求解框架，类似于EM算法，在求解概率模型中有很广泛的运用，是纵横江湖不可或缺的利器:-)

#### 几个概念：

#### 1 拓扑：

​        拓扑学研究不同空间在连续变化下保持不变的量。

​        拓扑是研究几何图形或空间在连续改变形状后还能保持不变的一些性质的一个学科。它只考虑物体间的位置关系而不考虑它们的形状和大小。

下面网址介绍了五个有趣的拓扑变换问题，特别特别地有趣。

http://www.matrix67.com/blog/archives/5140

#### **2 泛函：** 

​        简单地讲就是函数的函数，以函数集合为定义域的实值映射。

例如：要找到一个更加简单的函数D(Z)来近似P(Z|D)，同时问题转化为求解证据logP(Z)的下界L(Q)，或者L(Q(Z))。应该注意到L(Q)并非普通的函数，而是以整个函数为自变量的函数，这便是泛函。

我们先介绍一下什么是泛函，以及泛函取得极值的必要条件。

**泛函**

> 设对于(某一函数集合内的)任意一个函数y(x)，有另一个数J[y]与之对应，则称J[y]为y(x)的泛函。泛函可以看成是函数概念的推广。 这里的函数集合，即泛函的定义域，通常要求y(x) 满足一定的边界条件，并且具有连续的二阶导数．这样的y(x)称为可取函数。

**泛函不同于复合函数**，

> 例如g=g(f(x)); 对于后者，给定一个x值，仍然是有一个g值与之对应； 对于前者，则必须给出某一区间上的函数y(x)，才能得到一个泛函值J[y]。(定义在同一区间上的)函数不同，泛函值当然不同， 为了强调泛函值J[y]与函数y(x)之间的依赖关系，常常又把函数y(x)称为变量函数。

泛函的形式多种多样，通常可以积分形式：$J[y] = \int_{x_0}^{x_1} F(x,y,y')dx]$

**泛函的极值**

“当变量函数为y(x)时，泛函J[y]取极大值”的含义就是：对于极值函数y(x)及其“附近”的变量函数y(x)+δy(x)，恒有J[y+δy]≤J[y];

所谓函数y(x)+δy(x)在另一个函数y(x)的“附近”，指的是：

1. |δy(x)|<ε;
2. 有时还要求|(δy)′(x)|<ε.

这里的δy(x)称为函数y(x)的变分。

**Euler–Lagrange方程**

可以仿造函数极值必要条件的导出办法，导出泛函取极值的必要条件，这里不做严格的证明，直接给出。 泛函J[y]取到极大值的必要条件是一级变分δJ[y]为0，其微分形式一般为二阶常微分方程，即Euler-Largange方程：

$\frac{\partial F}{\partial}y - \frac{d}{d}x \frac{\partial F}{\partial}y' = 0$

#### 3 变分： 

​        求依赖于某些未知函数的**泛函数极值**的方法。与微分学中函数**极值问题**相类似。**最速降线问题、短程线问题和等周问题**等是古典变分学研究的典型问题。它最终寻求的是极值函数：它们使得泛函取得极大或极小值。在变分法中，我们研究的对象是高阶函数，它接受一个函数作为参数，并返回一个值。

​        变分法是处理函数的变量的数学领域，和处理数的函数的普通微积分相对。

​        上世纪90年代，变分推断在概率模型上得到迅速发展。

#### **4 KL散度：** 

来源：以下内容+KL详细公式过程来源：http://blog.csdn.net/aws3217150/article/details/57072827

作用：想要衡量两个概率模型有多大差异，可以利用KL-Divergence。

定义：两个概率密度函数为p(x)和q(x)之间的相对熵定义为:$D_{KL}(p||q) = \sum_xp(x)log \frac{p(x)}{q(x)}$

 性质：KL散度性质如下：

​            1 $D_{KL}(p||q) \ne D_{KL}(q||p) $ ；

​            2  $D_{KL}(p||q)\ge0 $,当且仅当p = q 时为0；

​            3 不满足三角不等式。

#### **5 变分贝叶斯：** 

**一、问题描述**

​        变分贝叶斯是一类用于贝叶斯估计和机器学习领域中近似计算复杂积分的技术。它主要应用于复杂的统计模型中，这种模型一般包括三类变量：观测变量(observed variables, data)，未知参数（parameters）和潜变量（latent variables）。在贝叶斯推断中，参数和潜变量统称为**不可观测变量**(unobserved variables)。

变分贝叶斯方法主要是两个目的：

> （1）近似不可观测变量的后验概率，以便通过这些变量作出统计推断。
>
> （2）对一个特定的模型，给出观测变量的边缘似然函数（或称为证据，evidence）的下界。主要用于模型的选择，认为模型的边缘似然值越高，则模型对数据拟合程度越好，该模型产生Data的概率也就越高。
>

​ 对于第一个目的（1）的解决方法:

> A. 蒙特卡洛模拟，特别是用Gibbs取样的MCMC方法，可以近似计算复杂的后验分布，能很好地应用到贝叶斯统计推断。此方法通过大量的样本估计真实的后验，因而近似结果带有一定的随机性。
>
> B. 与A不同的是，变分贝叶斯方法提供一种局部最优，但具有确定解的近似后验方法。从某种角度看，变分贝叶斯可以看做是EM算法的扩展，因为它也是采用极大后验估计(MAP),即用单个最有可能的参数值来代替完全贝叶斯估计。另外，变分贝叶斯也通过一组相互依赖（mutually dependent）的等式进行不断的迭代来获取最优解。

二、**形式化描述：**

> ​        重新考虑一个问题：1）有一组观测数据D，并且已知模型的形式，求参数与潜变量（或不可观测变量）Z={Z1,...,Zn} 的后验分布:P(Z|D)。
>
> ​        正如上文所描述的后验概率的形式通常是很复杂(Intractable)的,对于一种算法如果不能在多项式时间内求解，往往不是我们所考虑的。因而我们想能不能在误差允许的范围内，用更简单、容易理解(tractable)的数学形式Q(Z)来近似P(Z|D),即P(Z|D)≈Q(Z)。
>

由此引出如下两个问题：

> （1）     假设存在这样的Q(Z),那么如何度量Q(Z)与P(Z|D)之间的差异性（dissimilarity）？
>
> （2）     如何得到简单的Q(Z)?
>

对于问题一，幸运的是，我们不需要重新定义一个度量指标。在信息论中，已经存在描述两个随机分布之间距离的度量，即相对熵，或者称为Kullback-Leibler散度。

对于问题二，显然我们可以自主决定Q(Z)的分布，只要它足够简单，且与P(Z|D)接近。然而不可能每次都手工给出一个与P(Z|D)接近且简单的Q(Z)，其方法本身已经不具备可操作性。所以需要一种通用的形式帮助简化问题。那么数学形式复杂的原因是什么？在“模型的选择”部分，曾提到Occam's razor，认为一个模型的参数个数越多，那么模型复杂的概率越大;此外，如果参数之间具有相互依赖关系(mutually dependent)，那么通常很难对参数的边缘概率精确求解。幸运的是，统计物理学界很早就关注了高维概率函数与它的简单形式，并发展了平均场理论。简单讲就是：系统中个体的局部相互作用可以产生宏观层面较为稳定的行为。于是我们可以作出后验条件独立（posterior  independence）的假设。即：$\forall{i},p(Z|D) = p(Z_i|D)p(Z_{-i}|D)$

三、**Q分布与P分布的KL散度：**

$D_{KL}(Q||P)=\sum\limits_Z Q(Z)log\frac{Q(Z)}{P(Z|D)} = \sum\limits_ZQ(Z)log\frac{Q(Z)}{P(Z,D)}+logP(D)$

或者

$logP(D) = D_{KL}(Q||P)-\sum\limits_Z Q(Z)log\frac{Q(Z)}{P(Z, D)} = D_{KL}(Q||P) + L(Q)$

由于对数证据logP(D)被相应的Q所固定，为了使KL散度最小，则只要极大化L(Q)。通过选择合适的Q，使L(Q)便于计算和求极值。这样就可以得到后验P(Z|D)的近似解析表达式和对数证据(log evidence)的下界L(Q)，又称为变分自由能。

$L(Q)=-\sum\limits_ZQ(Z)log\frac{Q(Z)}{P(Z,D)}$

​           $=\sum\limits_Z {Q(Z)\log P(Z,D)}-\sum\limits_Z {Q(Z)\log Q(Z)}$

​           $={E_Q}[\log P(Z,D)]+H(Q)$



## 理解了变分贝叶斯后，接下来重头戏就是VAE详解：

以下内容的具体详解地址为：http://www.dengfanxin.cn/?p=334&sukey=72885186ae5c357d85d72afd35935fd5253f8a4e53d4ad672d5321379584a6b6e02e9713966e5f908dd7020bfa0c555f

一、 **问题描述：**

> 已知：有一个数据集DX(dataset)，每个数据也称为数据点X，这些数据点属于空间XS。
>
> 未知： 一组隐变量(latent variable)，假设有n个，$z_1, z_2 ,..., z_n$ ，这些隐变量属于空间ZS。则对于X中的每一个点都应该有n个隐变量$z_j$来决定。

注：由于一些误差等等，可能还有一些别的因素来决定每个X，那些因素可能十分巨大和繁多，但它们并不是形成DX的主因素，根据大数定理，这些所有因素产生的影响可以用高斯分布的概率密度函数来表示。我们没有能力关注全部的因素，我们只关心若干个可能重要的因素（$z_1, z_2 ,..., z_n$），这些因素的分布状况可以有各种假设，我们回头讨论它们概率分布问题，现在就假定我们对它们的具体分布情况一无所知，只知道它们处于ZS空间内。

​     如果一个数据集X对应的潜变量完全一样，那么这个数据集是一个单分类数据集；如果对应的潜变量是多个，这个数据集就是多分类数据集；如果对应的潜变量是一个连续的组合数据，则这个数据集就是一个有点分不清界限的复杂数据集。例如：我们这个数据集是一条线段的集合，线段的长度是唯一的潜变量，那么只要长度在一个范围内连续变化，那么这个集合里的线段就会分散的很均匀，你几乎没有办法区分开他们，也没法给他们分成几类，但如果这个长度值只能选择1,3,5，那么当你观察这个数据集的时候，你会发现他们会聚在三堆儿里。如果这个线段的生成完全依靠的是计算机，那么每一堆儿都是完全重合的，但如果是人画的，就可能因为误差，没法完全重合，这没法重合的部分就是我们说的其他复杂因素，我们通常用一个高斯分布来把它代表了。

二、 **形式化表示：**

> 假设两个变量，$z\in ZS$和$x\in XS$, 存在一个确定性函数族$f(z;\theta)$, 族中的每个函数由$\theta \in \Theta$ 唯一确定$f: ZS \times \Theta \rightarrow XS$，当θ固定，z是一个随机变量(概率密度函数为Pz(z))时，那么$f(z;\theta)$就是定义在XS上的随机变量x，对应的概率密度函数可以写成$g(x)$。
>
> 设一个数据集为DX,那么这个数据集存在的概率$P_t(DX)$，则根据贝叶斯公式有：
>
> ​                                                                   $P_t(DX) = \int P_{xz}(DX|z;\theta)P_z(z)dz$    (1)
>
> 其中，$P_{xz}(DX|z;\theta)$是我们新定义的概率密度函数，我们前面知道f是将z映射成x，而x又与DX有某种直接的关系，这个直接关系可以表示成$P_x(DX|x)$，那么 $P_t(DX) = \int P_{x}(DX|x)g(x)dx$, 这样我们就直接定义个$P_{xz}(DX|z;\theta)$ 来替换$P_{x}(DX|x)g(x)$，从而表示z与DX的关系了。

 注：公式(1)是潜变量和数据集DX之间的关系，该关系解释为：当隐变量按照某种规律存在时，就非常容易产生现在我们可观察到的这个数据集。现在需要做的就是----当假定有n个隐变量时，我们能够找到一个函数f，将隐变量的变化转换成x的变化，而这个x能够轻而易举地生成数据集DX。    

由前面的分析可知，影响数据集的因素有很多，最终反映成高斯函数，所以我们大胆假定$P_{xz}$是一个高斯分布的概率密度函数，即$P_{xz}(DX|z;\theta) = N(DX|f(x;\theta), \sigma^2 * I)$

目前，z的分布还是未知的，只是假设它为一个高斯分布。

三、 **VAE推断过程：**

既然z什么都不知道，则可以寻找一组新的变量w，让该w服从标准正态分布$N(0，I)$。然后w可以通过n个复杂函数，转化成z，其中w和z是一一对应的关系。该复杂函数可以用神经网络来模拟。

> 则推断过程为：
>
> ​                                           $w(N(0,I))--\frac{n个复杂函数}--z--\frac{  f_\theta  }--x--\frac{P_x(DX|x)}--DX$；
>
> 其中z的具体分布是什么，取值范围是多少我们也不用关心了，反正由一个神经网络去算。

则由该过程知，w与z是一一对应的关系，因此可以将z省去，由w直接通过复杂函数推出x，则以下将w称为z，因此接下来的z分布为$N(0，I)$。

> 到现在，可以获得的信息有：
>
> $P_z(z) = N(0,I)$
>
> $P_{xz}(DX|z;\theta) = N(DX|f( z ;\theta), \sigma^2 * I)$
>
> $P_t(DX) = \int P_{xz}(DX|z;\theta)P_z(z)dz$

接下来该专攻f了，由于f是一个神经网络，我们可以使用梯度下降了，但另一个关键点在于如何知道这个f生成的样本和DX更像？和要确定目标函数？接下来就是用KL散度来解决这些问题。

四、 **设定f神经网络的目标函数：**

由于理想分布$P_z(z|DX)$不好得到（各种误差等其他因素），因而可以定义个函数$Q(z|DX)$,我们可以根据DX计算出$Q(z|DX)$，让$Q(z|DX)$尽量与理想的$P_z(z|DX)$ 趋同。

函数$Q(z|DX)$的意义是：如果数据集DX发生了，$Q(z|DX)$就是z的概率密度函数，比如一个数字图像0，z隐式代表0的概率就很大，而那些代表1的概率就很小。如果有办法搞到这个Q的函数表示，我们就可以直接使用DX计算出z的最佳值了。

让$P_z(z|DX)$和$Q(z|DX)$趋同，就要使用KL散度（相对熵）：

> $P_z(z|DX)$和$Q(z|DX)$的KL散度为：
>
> $D[Q(z|DX)||P_z(z|DX)] = \int Q(z|DX)[logQ(z|DX)-logP_z(z|DX)]$
>
> 也可写成：
>
> $D[Q(z|DX)||P_z(z|DX)] = E_{z\sim Q}[logQ(z|DX)-logP_z(z|DX)]$
>
> 通过贝叶斯公式：
>
> $P_z(z|DX) = \frac{P(DX|z)P(z)}{P(DX)}$
>
> 接下来将$P_z$写为$P$：
>
> $D[Q(z|DX)||P(z|DX)] = E_{z\sim Q}[logQ(z|DX)-logP(DX|z)-logP(z)]+logP(DX)$
>
> 因为$logP(DX)$与z变量无关，直接就可以提出来，进而得到公式(2):
>
> $logP(DX) - D[Q(z|DX)||P(z|DX)]  = E_{z\sim Q}[logP(DX|z)]-D[Q(z|DX)||P(z)]$          (2)

公式2是VAE的核心公式，接下来分析该公式：

1. 公式左边第一项：我们的优化目标$P(DX)$；公式左边第二项：误差项，这个误差项反映了给定DX的情况下真实分布Q与理想分布P的相对熵，当Q完全符合理想分布时，该误差项为0.
2. 公式右边是我们可以使用梯度下降进行优化的：公式右边$Q(z|DX)$特别像一个$DX\to z$的编码器，公式右边$P(DX|z)$特别像一个$z\to DX$的解码器，这就是VAE架构被称为自编码器的原因。

现在将公式(2)拆分：

--左边第一项：$logP(DX)$

--左边第二项：$D(Q(z|DX)||P(z|DX))$

--右边第一项：$E_{z\sim Q}[logP(X|z)]$

--右边第二项：$D[Q(z|X)||P(z)]$

已知的公式为：

--$P(z) = N(0,I)$

--$P(DX|z) = N(DX|f( z ), \sigma^2 * I)$

--$Q(z|DX) = N(z|\mu(DX),\Sigma(DX))$



我们的目标是优化P(DX),但我们不知道他的分布，所以没办法优化，这意思是说我们没有任何先验知识。有了公式(2)，左边第二项是$P_z(z|DX)$和$Q(z|DX)$ 的相对熵，意味着DX发生时现实的分布应该与我们理想的分布趋同才对，所以整个左边都是我们的优化目标，只要左边越大越好，则因而右边的目标也是越大越好。

因而我们对准右边来优化：

右边第一项：$E_{z\sim Q}[logP(DX|z)]$ 就是根据真实的z来算出X的分布，即根据z重建X的过程，解码过程。

右边第二项：$D[Q(z|DX)||P(z)]$就是根据X重建z使其尽量趋近真实的z，其中P(z)是明确的N(0，I)分布，而$Q(z|DX)$ 是正态分布，即让$Q(z|DX)$ 趋向于标准正态分布，编码过程。

对公式(2)有了深入的理解后，接下来就是实现工作。

五、**实现**

针对公式(2)右边两项分别实现：

- 第二项是$Q(z|DX)$与N(0，I)的相对熵($KL(N(\mu，\Sigma)||N(0,I))$)，$X\to z$构成了编码器部分。

​       省略推导过程，要变成具体的神经网络和矩阵运算，将形式变化为：

​                                    $KL(N(\mu，\Sigma)||N(0,I)) = \frac{1}{2}\Sigma_i[-log(\Sigma_i)+\Sigma_i+\mu^2_i -1]$

​        KL可以计算了，然后就是编码器网络，$\mu(DX)$和$\Sigma(DX)$都使用神经网络来编码就可以了。 

- 第一项是$E_{z\sim Q}[logP(DX|z)]$ 表示依赖z重建出来的数据与X尽量地相同，$z\to X$重建X构成了解码器部分，整个重建的关键就是f函数，对我们来说就是建立一个解码器神经网络。

实现的流程图：

![img](http://www.dengfanxin.cn/wp-content/uploads/2016/11/222.png)

具体实现代码：https://github.com/vaxin/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/variational_autoencoder.py









