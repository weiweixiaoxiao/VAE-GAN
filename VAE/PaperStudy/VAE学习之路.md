# VAE学习之路

主要的论文Auto-Encoding Variational Bayes.

论文下载地址：http://arxiv.org/abs/1312.6114

“该论文提出一个融合变分贝叶斯（Variational Bayes）方法和神经网络的方法，这个方法可以用来构造生成模型的自编码器。”

这句话中提到--变分贝叶斯，先解释一下什么事变分贝叶斯。

## 变分贝叶斯

### 贝叶斯

 先验分布、后验分布、似然函数、贝叶斯公式

**先验分布**： 先验概率仅仅依赖于主观上的经验估计，也就是事先根据已有的知识的推断，先验概率就是没有经过实验验证的概率，根据已知进行的主观臆测。

​                                                     `如抛一枚硬币，在抛之前，主观推断P（正面朝上） = 0.5。`

**后验分布：** 后验概率是指在得到“结果”的信息后重新修正的概率，如贝叶斯公式中的。是“执果索因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。解释下来就是，在已知果（B）的前提下，得到重新修正的因（A）的概率P（A|B），称为A的后验概率，也即条件概率。后验概率可以通过贝叶斯公式求解。

**似然函数：** 在[数理统计学](http://zh.wikipedia.org/wiki/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%AD%A6)中，**似然函数**是一种关于[统计模型](http://zh.wikipedia.org/w/index.php?title=%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B&action=edit&redlink=1)中的[参数](http://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0)的[函数](http://zh.wikipedia.org/wiki/%E5%87%BD%E6%95%B0)，表示模型参数中的**似然性**。是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。

```
对于“一枚正反对称的硬币上抛十次”这种事件，我们可以问硬币落地时十次都是正面向上的“概率”是多少；
而对于“一枚硬币上抛十次，落地都是正面向上”这种事件，我们则可以问，这枚硬币正反面对称（也就是正反面概率均为0.5的概率）的“似然”程度是多少。
```



### 变分贝叶斯

**解释知识点最全的网址**：http://blog.huajh7.com/2013/03/06/variational-bayes/

​        通常在研究贝叶斯模型中，很多情况下我们关注的是如何求解后验概率(Posterior)，不幸的是，在实际模型中我们很难通过简单的贝叶斯理论求得后验概率的公式解，但是这并不影响我们对贝叶斯模型的爱——既然无法求得精确解，来个近似解在实际中也是可以接受的:-)。一般根据近似解的求解方式可以分为随机(Stochastic)近似方法（代表是MCMC，在[上一篇](http://blog.csdn.net/aws3217150/article/details/53840029)中我们提到的利用Gibbs Sampling训练LDA的模型便是一种），另外一种确定性(Deterministic)近似方法。本篇要介绍的变分推断便属于后者，一般情况下确定性近似方法会比随机近似方法更快和更容易判断收敛。变分贝叶斯推断是一种求解框架，类似于EM算法，在求解概率模型中有很广泛的运用，是纵横江湖不可或缺的利器:-)

#### 几个概念：

#### 1 拓扑：

#### **2 泛函：** 

​        简单地讲就是函数的函数，以函数集合为定义域的实值映射。

​        要找到一个更加简单的函数D(Z)来近似P(Z|D)，同时问题转化为求解证据logP(Z)的下界L(Q)，或者L(Q(Z))。应该注意到L(Q)并非普通的函数，而是以整个函数为自变量的函数，这便是泛函。我们先介绍一下什么是泛函，以及泛函取得极值的必要条件。

**泛函**

> 设对于(某一函数集合内的)任意一个函数y(x)，有另一个数J[y]与之对应，则称J[y]为y(x)的泛函。泛函可以看成是函数概念的推广。 这里的函数集合，即泛函的定义域，通常要求y(x) 满足一定的边界条件，并且具有连续的二阶导数．这样的y(x)称为可取函数。

**泛函不同于复合函数**，

> 例如g=g(f(x)); 对于后者，给定一个x值，仍然是有一个g值与之对应； 对于前者，则必须给出某一区间上的函数y(x)，才能得到一个泛函值J[y]。(定义在同一区间上的)函数不同，泛函值当然不同， 为了强调泛函值J[y]与函数y(x)之间的依赖关系，常常又把函数y(x)称为变量函数。

泛函的形式多种多样，通常可以积分形式：![3](C:\Users\侯丽微\Desktop\截图\3.png)

##### 泛函取极值的必要条件

**泛函的极值**

“当变量函数为y(x)时，泛函J[y]取极大值”的含义就是：对于极值函数y(x)及其“附近”的变量函数y(x)+δy(x)，恒有J[y+δy]≤J[y];

所谓函数y(x)+δy(x)在另一个函数y(x)的“附近”，指的是：

1. |δy(x)|<ε;
2. 有时还要求|(δy)′(x)|<ε.

这里的δy(x)称为函数y(x)的变分。

**Euler–Lagrange方程**

可以仿造函数极值必要条件的导出办法，导出泛函取极值的必要条件，这里不做严格的证明，直接给出。 泛函J[y]取到极大值的必要条件是一级变分δJ[y]为0，其微分形式一般为二阶常微分方程，即Euler-Largange方程：

![4](C:\Users\侯丽微\Desktop\截图\4.png)

![5](C:\Users\侯丽微\Desktop\截图\5.png)

#### 3 变分： 

​        求依赖于某些未知函数的**泛函数极值**的方法。与微分学中函数**极值问题**相类似。**最速降线问题、短程线问题和等周问题**等是古典变分学研究的典型问题。它最终寻求的是极值函数：它们使得泛函取得极大或极小值。在变分法中，我们研究的对象是高阶函数，它接受一个函数作为参数，并返回一个值。

​        变分法是处理函数的变量的数学领域，和处理数的函数的普通微积分相对。

​        上世纪90年代，变分推断在概率模型上得到迅速发展。

#### **4 KL散度：** 

以下内容+KL详细公式过程来源：http://blog.csdn.net/aws3217150/article/details/57072827

想要衡量两个概率模型有多大差异，可以利用KL-Divergence。

![2](C:\Users\侯丽微\Desktop\截图\2.png)

#### **5 变分贝叶斯：** 

​        变分贝叶斯是一类用于贝叶斯估计和机器学习领域中近似计算复杂积分的技术。它主要应用于复杂的统计模型中，这种模型一般包括三类变量：观测变量(observed variables, data)，未知参数（parameters）和潜变量（latent variables）。在贝叶斯推断中，参数和潜变量统称为**不可观测变量**(unobserved variables)。

变分贝叶斯方法主要是两个目的：

（1）近似不可观测变量的后验概率，以便通过这些变量作出统计推断。

（2）对一个特定的模型，给出观测变量的边缘似然函数（或称为证据，evidence）的下界。主要用于模型的选择，认为模型的边缘似然值越高，则模型对数据拟合程度越好，该模型产生Data的概率也就越高。

​        对于第一个目的，蒙特卡洛模拟，特别是用Gibbs取样的MCMC方法，可以近似计算复杂的后验分布，能很好地应用到贝叶斯统计推断。此方法通过大量的样本估计真实的后验，因而近似结果带有一定的随机性。与此不同的是，变分贝叶斯方法提供一种局部最优，但具有确定解的近似后验方法。

​        从某种角度看，变分贝叶斯可以看做是EM算法的扩展，因为它也是采用极大后验估计(MAP),即用单个最有可能的参数值来代替完全贝叶斯估计。另外，变分贝叶斯也通过一组相互依赖（mutually dependent）的等式进行不断的迭代来获取最优解。

二、问题描述

重新考虑一个问题：1）有一组观测数据D，并且已知模型的形式，求参数与潜变量（或不可观测变量）Z={Z1,...,Zn} 的后验分布:P(Z|D)。

正如上文所描述的后验概率的形式通常是很复杂(Intractable)的,对于一种算法如果不能在多项式时间内求解，往往不是我们所考虑的。因而我们想能不能在误差允许的范围内，用更简单、容易理解(tractable)的数学形式Q(Z)来近似P(Z|D),即P(Z|D)≈Q(Z)。

由此引出如下两个问题：

（1）     假设存在这样的Q(Z),那么如何度量Q(Z)与P(Z|D)之间的差异性（dissimilarity）？

（2）     如何得到简单的Q(Z)?

对于问题一，幸运的是，我们不需要重新定义一个度量指标。在信息论中，已经存在描述两个随机分布之间距离的度量，即相对熵，或者称为Kullback-Leibler散度。

对于问题二，显然我们可以自主决定Q(Z)的分布，只要它足够简单，且与P(Z|D)接近。然而不可能每次都手工给出一个与P(Z|D)接近且简单的Q(Z)，其方法本身已经不具备可操作性。所以需要一种通用的形式帮助简化问题。那么数学形式复杂的原因是什么？在“模型的选择”部分，曾提到Occam's razor，认为一个模型的参数个数越多，那么模型复杂的概率越大;此外，如果参数之间具有相互依赖关系(mutually dependent)，那么通常很难对参数的边缘概率精确求解。

幸运的是，统计物理学界很早就关注了高维概率函数与它的简单形式，并发展了平均场理论。简单讲就是：系统中个体的局部相互作用可以产生宏观层面较为稳定的行为。于是我们可以作出后验条件独立（posterior  independence）的假设。即，![1](C:\Users\侯丽微\Desktop\截图\1.png)

···························································································································································································

 以下图片内容来自http://blog.csdn.net/aws3217150/article/details/57072827

![2](C:\Users\侯丽微\Desktop\2.png)

![1](C:\Users\侯丽微\Desktop\1.png)

图中绿色图是P的分布，图(a)红色线利用通过最小化KL(Q||P)也就是变分推断获得的结果，图(b)红色线是通过最小化KL(P||Q)的结果。如果选择最小化KL(P||Q)，那么其实是对应于另外一种近似框架——Expectation Propagation，超出本篇要讨论的，暂且搁置。

那么既然我们有了目标对象——最小化KL(Q||P)，接下来就是如何求得最小化时的Q了。我们将公式稍微变换一下： http://blog.csdn.net/aws3217150/article/details/57072827 有公式详解。

![3](C:\Users\侯丽微\Desktop\3.png)



## 理解了变分贝叶斯后，接下来重头戏就是VAE详解：

